diff --git a/README.md b/README.md
index e96c9177..5f310564 100644
--- a/README.md
+++ b/README.md
@@ -2,12 +2,6 @@
   <img width="1000" src="./img/full-logo.svg">
 </p>
 
-Upcoming Tutorial
-===============================
-We will be presenting [a new tutorial](https://sites.google.com/berkeley.edu/gemmini-tutorial-mlsys-2022) for Gemmini at MLSys 2022, on August 29th, 2022.
-
-If you would like to attend, **then please register online** [at this link](https://docs.google.com/forms/d/1bdIXegBkEMJY88YuD80HN40haZ9tx_bZgmaN3FON5DI/edit). We're looking forward to meeting you all!
-
 Gemmini
 ====================================
 
@@ -52,7 +46,7 @@ SPIKE_HASH=$(cat SPIKE.hash)
 
 cd -
 cd toolchains/esp-tools/riscv-isa-sim/build
-git checkout $SPIKE_HASH
+git fetch && git checkout $SPIKE_HASH
 make && make install
 
 # The final step is only necessary if you want to run MIDAS simulations with
diff --git a/src/main/scala/gemmini/AccumulatorMem.scala b/src/main/scala/gemmini/AccumulatorMem.scala
index dd5ed821..c664bd0f 100644
--- a/src/main/scala/gemmini/AccumulatorMem.scala
+++ b/src/main/scala/gemmini/AccumulatorMem.scala
@@ -230,7 +230,7 @@ class AccumulatorMem[T <: Data, U <: Data](
       val wmask = Mux1H(w_q_head.asBools, w_q.map(_.mask))
       val waddr = Mux1H(w_q_head.asBools, w_q.map(_.addr))
       when (wen) {
-        w_q_head := (w_q_head << 1).asUInt() | w_q_head(nEntries-1)
+        w_q_head := (w_q_head << 1).asUInt | w_q_head(nEntries-1)
         for (i <- 0 until nEntries) {
           when (w_q_head(i)) {
             w_q(i).valid := false.B
@@ -243,7 +243,7 @@ class AccumulatorMem[T <: Data, U <: Data](
       when (w_q_push) {
         assert(!w_q_full || wen, "we ran out of acc-sub-bank write q entries")
 
-        w_q_tail := (w_q_tail << 1).asUInt() | w_q_tail(nEntries-1)
+        w_q_tail := (w_q_tail << 1).asUInt | w_q_tail(nEntries-1)
         for (i <- 0 until nEntries) {
           when (w_q_tail(i)) {
             w_q(i).valid := true.B
@@ -334,7 +334,7 @@ class AccumulatorMem[T <: Data, U <: Data](
   io.write.ready := !block_write_req &&
     !pipelined_writes.map(r => r.valid && r.bits.addr === io.write.bits.addr && io.write.bits.acc).reduce(_||_)
 
-  when (reset.asBool()) {
+  when (reset.asBool) {
     pipelined_writes.foreach(_.valid := false.B)
   }
 
diff --git a/src/main/scala/gemmini/AccumulatorScale.scala b/src/main/scala/gemmini/AccumulatorScale.scala
index 1fdd15fa..bf98a144 100644
--- a/src/main/scala/gemmini/AccumulatorScale.scala
+++ b/src/main/scala/gemmini/AccumulatorScale.scala
@@ -175,7 +175,7 @@ class AccumulatorScale[T <: Data, U <: Data](
           completed_masks(i).foreach(_ := false.B)
         }
       }
-      tail_oh := (tail_oh << 1).asUInt() | tail_oh(nEntries-1)
+      tail_oh := (tail_oh << 1).asUInt | tail_oh(nEntries-1)
     }
 
     val inputs = Seq.fill(width*nEntries) { Wire(Decoupled(new AccScaleDataWithIndex(t, scale_t)(ev))) }
@@ -282,10 +282,10 @@ object AccumulatorScale {
     // qln2_inv / S / (2 ** 16) = 1 / ln2
     // q * qln2_inv = x / S / ln2 * S * (2 ** 16) = x / ln2 * (2 ** 16)
     val neg_q_iexp = neg(q)
-    val z_iexp = (neg_q_iexp * qln2_inv).asUInt().do_>>(16).asTypeOf(q) // q is non-positive
+    val z_iexp = (neg_q_iexp * qln2_inv).asUInt.do_>>(16).asTypeOf(q) // q is non-positive
     val qp_iexp = q.mac(z_iexp, qln2).withWidthOf(q)
     val q_poly_iexp = qc.mac(qp_iexp + qb, qp_iexp + qb).withWidthOf(q)
     // we dont want a rounding shift
-    (q_poly_iexp.asUInt().do_>>(z_iexp.asUInt()(5, 0))).asTypeOf(q)
+    (q_poly_iexp.asUInt.do_>>(z_iexp.asUInt(5, 0))).asTypeOf(q)
   }}
 
diff --git a/src/main/scala/gemmini/Arithmetic.scala b/src/main/scala/gemmini/Arithmetic.scala
index cdd36396..c6792578 100644
--- a/src/main/scala/gemmini/Arithmetic.scala
+++ b/src/main/scala/gemmini/Arithmetic.scala
@@ -62,12 +62,12 @@ object Arithmetic {
 
         // TODO Do we need to explicitly handle the cases where "u" is a small number (like 0)? What is the default behavior here?
         val point_five = Mux(u === 0.U, 0.U, self(u - 1.U))
-        val zeros = Mux(u <= 1.U, 0.U, self.asUInt() & ((1.U << (u - 1.U)).asUInt() - 1.U)) =/= 0.U
+        val zeros = Mux(u <= 1.U, 0.U, self.asUInt & ((1.U << (u - 1.U)).asUInt - 1.U)) =/= 0.U
         val ones_digit = self(u)
 
         val r = point_five & (zeros | ones_digit)
 
-        (self >> u).asUInt() + r
+        (self >> u).asUInt + r
       }
 
       override def >(t: UInt): Bool = self > t
@@ -99,19 +99,19 @@ object Arithmetic {
 
         // TODO Do we need to explicitly handle the cases where "u" is a small number (like 0)? What is the default behavior here?
         val point_five = Mux(u === 0.U, 0.U, self(u - 1.U))
-        val zeros = Mux(u <= 1.U, 0.U, self.asUInt() & ((1.U << (u - 1.U)).asUInt() - 1.U)) =/= 0.U
+        val zeros = Mux(u <= 1.U, 0.U, self.asUInt & ((1.U << (u - 1.U)).asUInt - 1.U)) =/= 0.U
         val ones_digit = self(u)
 
-        val r = (point_five & (zeros | ones_digit)).asBool()
+        val r = (point_five & (zeros | ones_digit)).asBool
 
-        (self >> u).asSInt() + Mux(r, 1.S, 0.S)
+        (self >> u).asSInt + Mux(r, 1.S, 0.S)
       }
 
       override def >(t: SInt): Bool = self > t
 
       override def withWidthOf(t: SInt) = {
         if (self.getWidth >= t.getWidth)
-          self(t.getWidth-1, 0).asSInt()
+          self(t.getWidth-1, 0).asSInt
         else {
           val sign_bits = t.getWidth - self.getWidth
           val sign = self(self.getWidth-1)
@@ -122,7 +122,7 @@ object Arithmetic {
       override def clippedToWidthOf(t: SInt): SInt = {
         val maxsat = ((1 << (t.getWidth-1))-1).S
         val minsat = (-(1 << (t.getWidth-1))).S
-        MuxCase(self, Seq((self > maxsat) -> maxsat, (self < minsat) -> minsat))(t.getWidth-1, 0).asSInt()
+        MuxCase(self, Seq((self > maxsat) -> maxsat, (self < minsat) -> minsat))(t.getWidth-1, 0).asSInt
       }
 
       override def relu: SInt = Mux(self >= 0.S, self, 0.S)
@@ -144,7 +144,7 @@ object Arithmetic {
         def sin_to_float(x: SInt) = {
           val in_to_rec_fn = Module(new INToRecFN(intWidth = self.getWidth, expWidth, sigWidth))
           in_to_rec_fn.io.signedIn := true.B
-          in_to_rec_fn.io.in := x.asUInt()
+          in_to_rec_fn.io.in := x.asUInt
           in_to_rec_fn.io.roundingMode := consts.round_minMag // consts.round_near_maxMag
           in_to_rec_fn.io.detectTininess := consts.tininess_afterRounding
 
@@ -167,7 +167,7 @@ object Arithmetic {
           rec_fn_to_in.io.in := x
           rec_fn_to_in.io.roundingMode := consts.round_minMag // consts.round_near_maxMag
 
-          rec_fn_to_in.io.out.asSInt()
+          rec_fn_to_in.io.out.asSInt
         }
 
         val self_rec = sin_to_float(self)
@@ -207,7 +207,7 @@ object Arithmetic {
         def in_to_float(x: SInt) = {
           val in_to_rec_fn = Module(new INToRecFN(intWidth = self.getWidth, expWidth, sigWidth))
           in_to_rec_fn.io.signedIn := true.B
-          in_to_rec_fn.io.in := x.asUInt()
+          in_to_rec_fn.io.in := x.asUInt
           in_to_rec_fn.io.roundingMode := consts.round_minMag // consts.round_near_maxMag
           in_to_rec_fn.io.detectTininess := consts.tininess_afterRounding
 
@@ -220,7 +220,7 @@ object Arithmetic {
           rec_fn_to_in.io.in := x
           rec_fn_to_in.io.roundingMode := consts.round_minMag // consts.round_near_maxMag
 
-          rec_fn_to_in.io.out.asSInt()
+          rec_fn_to_in.io.out.asSInt
         }
 
         val self_rec = in_to_float(self)
@@ -255,7 +255,7 @@ object Arithmetic {
           def in_to_float(x: SInt) = {
             val in_to_rec_fn = Module(new INToRecFN(intWidth = self.getWidth, expWidth, sigWidth))
             in_to_rec_fn.io.signedIn := true.B
-            in_to_rec_fn.io.in := x.asUInt()
+            in_to_rec_fn.io.in := x.asUInt
             in_to_rec_fn.io.roundingMode := consts.round_near_even // consts.round_near_maxMag
             in_to_rec_fn.io.detectTininess := consts.tininess_afterRounding
 
@@ -291,7 +291,7 @@ object Arithmetic {
           def in_to_float(x: SInt) = {
             val in_to_rec_fn = Module(new INToRecFN(intWidth = self.getWidth, expWidth, sigWidth))
             in_to_rec_fn.io.signedIn := true.B
-            in_to_rec_fn.io.in := x.asUInt()
+            in_to_rec_fn.io.in := x.asUInt
             in_to_rec_fn.io.roundingMode := consts.round_near_even // consts.round_near_maxMag
             in_to_rec_fn.io.detectTininess := consts.tininess_afterRounding
 
@@ -304,7 +304,7 @@ object Arithmetic {
             rec_fn_to_in.io.in := x
             rec_fn_to_in.io.roundingMode := consts.round_minMag
 
-            rec_fn_to_in.io.out.asSInt()
+            rec_fn_to_in.io.out.asSInt
           }
 
           val self_rec = in_to_float(self)
diff --git a/src/main/scala/gemmini/BeatMerger.scala b/src/main/scala/gemmini/BeatMerger.scala
index a6a67dab..e8f22b2a 100644
--- a/src/main/scala/gemmini/BeatMerger.scala
+++ b/src/main/scala/gemmini/BeatMerger.scala
@@ -59,11 +59,11 @@ class BeatMerger[U <: Data](beatBits: Int, maxShift: Int, spadWidth: Int, accWid
   }
 
   val last_sending = bytesSent_next === req.bits.bytes_to_read
-  val last_reading = beatBytes.U >= (1.U << req.bits.lg_len_req).asUInt() - bytesRead
+  val last_reading = beatBytes.U >= (1.U << req.bits.lg_len_req).asUInt - bytesRead
 
   io.req.ready := !req.valid
 
-  io.in.ready := io.req.fire || (req.valid && bytesRead =/= (1.U << req.bits.lg_len_req).asUInt())
+  io.in.ready := io.req.fire || (req.valid && bytesRead =/= (1.U << req.bits.lg_len_req).asUInt)
 
   io.out.valid := req.valid && usefulBytesRead > bytesSent && (usefulBytesRead - bytesSent >= rowBytes ||
     usefulBytesRead === req.bits.bytes_to_read)
@@ -90,7 +90,7 @@ class BeatMerger[U <: Data](beatBits: Int, maxShift: Int, spadWidth: Int, accWid
   io.out.bits.accumulate := req.bits.accumulate
   io.out.bits.has_acc_bitwidth := req.bits.has_acc_bitwidth
 
-  when (bytesRead === (1.U << req.bits.lg_len_req).asUInt() &&
+  when (bytesRead === (1.U << req.bits.lg_len_req).asUInt &&
     bytesSent === req.bits.bytes_to_read) {
     req.pop()
   }
@@ -98,7 +98,7 @@ class BeatMerger[U <: Data](beatBits: Int, maxShift: Int, spadWidth: Int, accWid
   when (io.out.fire) {
     bytesSent := bytesSent_next
 
-    when (last_sending && bytesRead === (1.U << req.bits.lg_len_req).asUInt()) {
+    when (last_sending && bytesRead === (1.U << req.bits.lg_len_req).asUInt) {
       req.pop()
       io.req.ready := true.B
     }
@@ -116,16 +116,16 @@ class BeatMerger[U <: Data](beatBits: Int, maxShift: Int, spadWidth: Int, accWid
     val current_usefulBytesRead = Mux(io.req.fire, 0.U, usefulBytesRead)
     val current_shift = Mux(io.req.fire, io.req.bits.shift, req.bits.shift)
     val current_lg_len_req = Mux(io.req.fire, io.req.bits.lg_len_req, req.bits.lg_len_req)
-    val current_len_req = (1.U << current_lg_len_req).asUInt()
+    val current_len_req = (1.U << current_lg_len_req).asUInt
 
     when (current_shift - current_bytesDiscarded <= beatBytes.U /* &&
       current_bytesRead < current_len_req */
     ) {
       val rshift = (current_shift - current_bytesDiscarded) * 8.U // in bits
       val lshift = current_usefulBytesRead * 8.U // in bits
-      val mask = (~(((~0.U(beatBits.W)) >> rshift) << lshift)).asUInt()
+      val mask = (~(((~0.U(beatBits.W)) >> rshift) << lshift)).asUInt
 
-      buffer := (buffer & mask) | ((io.in.bits >> rshift) << lshift).asUInt()
+      buffer := (buffer & mask) | ((io.in.bits >> rshift) << lshift).asUInt
     }
 
     bytesRead := satAdd(current_bytesRead, beatBytes.U, current_len_req)
@@ -135,7 +135,7 @@ class BeatMerger[U <: Data](beatBits: Int, maxShift: Int, spadWidth: Int, accWid
     }
   }
 
-  when (reset.asBool()) {
+  when (reset.asBool) {
     req.valid := false.B
   }
 }
diff --git a/src/main/scala/gemmini/DMA.scala b/src/main/scala/gemmini/DMA.scala
index 71148b67..729e17c7 100644
--- a/src/main/scala/gemmini/DMA.scala
+++ b/src/main/scala/gemmini/DMA.scala
@@ -380,7 +380,7 @@ class StreamWriter[T <: Data: Arithmetic](nXacts: Int, beatBits: Int, maxBytes:
     // TODO use the same register to hold data_blocks and data_single_block, so that this Mux here is not necessary
     val data_blocks = Reg(Vec(maxBlocks, UInt((inputTypeRowBytes * 8).W)))
     val data_single_block = Reg(UInt(dataWidth.W)) // For data that's just one-block-wide
-    val data = Mux(req.block === 0.U, data_single_block, data_blocks.asUInt())
+    val data = Mux(req.block === 0.U, data_single_block, data_blocks.asUInt)
 
     val bytesSent = Reg(UInt(log2Ceil((dataBytes max maxBytes)+1).W))  // TODO this only needs to count up to (dataBytes/aligned_to), right?
     val bytesLeft = req.len - bytesSent
@@ -390,9 +390,9 @@ class StreamWriter[T <: Data: Arithmetic](nXacts: Int, beatBits: Int, maxBytes:
     val xactId = OHToUInt(xactOnehot)
 
     val xactBusy_fire = WireInit(false.B)
-    val xactBusy_add = Mux(xactBusy_fire, (1.U << xactId).asUInt(), 0.U)
-    val xactBusy_remove = ~Mux(tl.d.fire, (1.U << tl.d.bits.source).asUInt(), 0.U)
-    xactBusy := (xactBusy | xactBusy_add) & xactBusy_remove.asUInt()
+    val xactBusy_add = Mux(xactBusy_fire, (1.U << xactId).asUInt, 0.U)
+    val xactBusy_remove = ~Mux(tl.d.fire, (1.U << tl.d.bits.source).asUInt, 0.U)
+    xactBusy := (xactBusy | xactBusy_add) & xactBusy_remove.asUInt
 
     val state_machine_ready_for_req = WireInit(state === s_idle)
     io.req.ready := state_machine_ready_for_req
@@ -482,15 +482,15 @@ class StreamWriter[T <: Data: Arithmetic](nXacts: Int, beatBits: Int, maxBytes:
       fromSource = RegEnableThru(xactId, state === s_writing_new_block),
       toAddress = 0.U,
       lgSize = lg_write_size,
-      data = (data >> (bytesSent * 8.U)).asUInt()
+      data = (data >> (bytesSent * 8.U)).asUInt
     )._2
 
     val putPartial = edge.Put(
       fromSource = RegEnableThru(xactId, state === s_writing_new_block),
       toAddress = 0.U,
       lgSize = lg_write_size,
-      data = ((data >> (bytesSent * 8.U)) << (write_shift * 8.U)).asUInt(),
-      mask = write_mask.asUInt()
+      data = ((data >> (bytesSent * 8.U)) << (write_shift * 8.U)).asUInt,
+      mask = write_mask.asUInt
     )._2
 
     class TLBundleAWithInfo extends Bundle {
@@ -501,7 +501,7 @@ class StreamWriter[T <: Data: Arithmetic](nXacts: Int, beatBits: Int, maxBytes:
 
     val untranslated_a = Wire(Decoupled(new TLBundleAWithInfo))
     xactBusy_fire := untranslated_a.fire && state === s_writing_new_block
-    untranslated_a.valid := (state === s_writing_new_block || state === s_writing_beats) && !xactBusy.andR()
+    untranslated_a.valid := (state === s_writing_new_block || state === s_writing_beats) && !xactBusy.andR
     untranslated_a.bits.tl_a := Mux(write_full, putFull, putPartial)
     untranslated_a.bits.vaddr := write_vaddr
     untranslated_a.bits.status := req.status
@@ -543,7 +543,7 @@ class StreamWriter[T <: Data: Arithmetic](nXacts: Int, beatBits: Int, maxBytes:
     tl.a.bits := translate_q.io.deq.bits.tl_a
     tl.a.bits.address := RegEnableThru(io.tlb.resp.paddr, RegNext(io.tlb.req.fire))
 
-    tl.d.ready := xactBusy.orR()
+    tl.d.ready := xactBusy.orR
 
     when (untranslated_a.fire) {
       when (state === s_writing_new_block) {
@@ -588,7 +588,7 @@ class StreamWriter[T <: Data: Arithmetic](nXacts: Int, beatBits: Int, maxBytes:
         val v1 = io.req.bits.data.asTypeOf(Vec(cols, inputType))
         val v2 = data_single_block.asTypeOf(Vec(cols, inputType))
         val m = v1.zip(v2)
-        VecInit(m.zipWithIndex.map{case ((x, y), i) => if (i < block_cols) maxOf(x, y) else y}).asUInt()
+        VecInit(m.zipWithIndex.map{case ((x, y), i) => if (i < block_cols) maxOf(x, y) else y}).asUInt
       }
 
       req := io.req.bits
diff --git a/src/main/scala/gemmini/DMACommandTracker.scala b/src/main/scala/gemmini/DMACommandTracker.scala
index 9d4f71e6..a2b5df32 100644
--- a/src/main/scala/gemmini/DMACommandTracker.scala
+++ b/src/main/scala/gemmini/DMACommandTracker.scala
@@ -93,7 +93,7 @@ class DMACommandTracker[T <: Data](val nCmds: Int, val maxBytes: Int, tag_t: =>
     cmds(io.cmd_completed.bits.cmd_id).valid := false.B
   }
 
-  when (reset.asBool()) {
+  when (reset.asBool) {
     cmds.foreach(_.init())
   }
 }
diff --git a/src/main/scala/gemmini/DSEConfigs.scala b/src/main/scala/gemmini/DSEConfigs.scala
index 3ed92c7c..257721ca 100644
--- a/src/main/scala/gemmini/DSEConfigs.scala
+++ b/src/main/scala/gemmini/DSEConfigs.scala
@@ -51,12 +51,12 @@ object DSEBaseConfig {
 
         // TODO Do we need to explicitly handle the cases where "u" is a small number (like 0)? What is the default behavior here?
         val point_five = Mux(u === 0.U, 0.U, t(u - 1.U))
-        val zeros = Mux(u <= 1.U, 0.U, t.asUInt() & ((1.U << (u - 1.U)).asUInt() - 1.U)) =/= 0.U
+        val zeros = Mux(u <= 1.U, 0.U, t.asUInt & ((1.U << (u - 1.U)).asUInt - 1.U)) =/= 0.U
         val ones_digit = t(u)
 
-        val r = (point_five & (zeros | ones_digit)).asBool()
+        val r = (point_five & (zeros | ones_digit)).asBool
 
-        (t >> u).asSInt() + Mux(r, 1.S, 0.S)
+        (t >> u).asSInt + Mux(r, 1.S, 0.S)
       }, 0, UInt(8.W), -1)),
     acc_read_full_width = true,
     acc_read_small_width = true,
diff --git a/src/main/scala/gemmini/ExecuteController.scala b/src/main/scala/gemmini/ExecuteController.scala
index 2ef7fa3f..65add720 100644
--- a/src/main/scala/gemmini/ExecuteController.scala
+++ b/src/main/scala/gemmini/ExecuteController.scala
@@ -801,8 +801,8 @@ class ExecuteController[T <: Data, U <: Data, V <: Data](xLen: Int, tagWidth: In
   mesh_cntl_signals_q.io.enq.bits.first := !a_fire_started && !b_fire_started && !d_fire_started
 
   val readData = VecInit(io.srams.read.map(_.resp.bits.data))
-  val accReadData = if (ex_read_from_acc) VecInit(io.acc.read_resp.map(_.bits.data.asUInt())) else readData
-  val im2ColData = io.im2col.resp.bits.a_im2col.asUInt()
+  val accReadData = if (ex_read_from_acc) VecInit(io.acc.read_resp.map(_.bits.data.asUInt)) else readData
+  val im2ColData = io.im2col.resp.bits.a_im2col.asUInt
 
   val readValid = VecInit(io.srams.read.map(bank => ex_read_from_spad.B && bank.resp.valid && !bank.resp.bits.fromDMA))
   val accReadValid = VecInit(io.acc.read_resp.map(bank => ex_read_from_acc.B && bank.valid && !bank.bits.fromDMA))
@@ -933,7 +933,7 @@ class ExecuteController[T <: Data, U <: Data, V <: Data](xLen: Int, tagWidth: In
     if (ex_write_to_spad) {
       io.srams.write(i).en := start_array_outputting && w_bank === i.U && !write_to_acc && !is_garbage_addr && write_this_row
       io.srams.write(i).addr := w_row
-      io.srams.write(i).data := activated_wdata.asUInt()
+      io.srams.write(i).data := activated_wdata.asUInt
       io.srams.write(i).mask := w_mask.flatMap(b => Seq.fill(inputType.getWidth / (aligned_to * 8))(b))
     } else {
       io.srams.write(i).en := false.B
@@ -993,7 +993,7 @@ class ExecuteController[T <: Data, U <: Data, V <: Data](xLen: Int, tagWidth: In
     complete_bits_count := complete_bits_count + 1.U
   }
 
-  when (reset.asBool()) {
+  when (reset.asBool) {
     // pending_completed_rob_id.valid := false.B
     pending_completed_rob_ids.foreach(_.valid := false.B)
   }
diff --git a/src/main/scala/gemmini/FrontendTLB.scala b/src/main/scala/gemmini/FrontendTLB.scala
index 6e7168e9..63a41496 100644
--- a/src/main/scala/gemmini/FrontendTLB.scala
+++ b/src/main/scala/gemmini/FrontendTLB.scala
@@ -119,7 +119,7 @@ class FrontendTLB(nClients: Int, entries: Int, maxSize: Int, use_tlb_register_fi
     val last_translated_vpn = RegInit(0.U(vaddrBits.W))
     val last_translated_ppn = RegInit(0.U(paddrBits.W))
 
-    val l0_tlb_hit = last_translated_valid && ((client.req.bits.tlb_req.vaddr >> pgIdxBits).asUInt() === (last_translated_vpn >> pgIdxBits).asUInt())
+    val l0_tlb_hit = last_translated_valid && ((client.req.bits.tlb_req.vaddr >> pgIdxBits).asUInt === (last_translated_vpn >> pgIdxBits).asUInt)
     val l0_tlb_paddr = Cat(last_translated_ppn >> pgIdxBits, client.req.bits.tlb_req.vaddr(pgIdxBits-1,0))
 
     val tlb = if (use_shared_tlb) tlbs.head else tlbs(i)
diff --git a/src/main/scala/gemmini/Im2Col.scala b/src/main/scala/gemmini/Im2Col.scala
index a317902b..65c4dbd4 100644
--- a/src/main/scala/gemmini/Im2Col.scala
+++ b/src/main/scala/gemmini/Im2Col.scala
@@ -90,12 +90,12 @@ class Im2Col[T <: Data, U <: Data, V <: Data](config: GemminiArrayConfig[T, U, V
 
 
   //how much horizonal turn we have to compute (input_channel*kernel_dim/16)
-  //val turn = Mux(im2col_width(3,0) === 0.U, (im2col_width >> (log2Up(block_size)).U).asUInt(), (im2col_width >> (log2Up(block_size)).U).asUInt + 1.U)
+  //val turn = Mux(im2col_width(3,0) === 0.U, (im2col_width >> (log2Up(block_size)).U).asUInt, (im2col_width >> (log2Up(block_size)).U).asUInt + 1.U)
   val turn = filter_dim2//Mux(channel(3,0) === 0.U, filter_dim2*channel(6, 4), filter_dim2*channel(6, 4) + 1.U)
 
   //Seah: added for more than 16 rows of output
   //how much vertical turn we have to compute (output_dim/16)
-  //val row_turn = Mux(output_dim(3,0) === 0.U, (output_dim >> (log2Up(block_size)).U).asUInt - 1.U, (output_dim >> (log2Up(block_size)).U).asUInt()) //im2col height
+  //val row_turn = Mux(output_dim(3,0) === 0.U, (output_dim >> (log2Up(block_size)).U).asUInt - 1.U, (output_dim >> (log2Up(block_size)).U).asUInt) //im2col height
   val row_turn = io.req.bits.row_turn
   val row_left = io.req.bits.row_left
 
diff --git a/src/main/scala/gemmini/InstructionCompression.scala b/src/main/scala/gemmini/InstructionCompression.scala
index 96bc77ee..fe6cd3d9 100644
--- a/src/main/scala/gemmini/InstructionCompression.scala
+++ b/src/main/scala/gemmini/InstructionCompression.scala
@@ -40,7 +40,7 @@ class InstCompressor(implicit p: Parameters) extends Module {
     buf(waddr).push(io.in.bits)
   }
 
-  when (reset.asBool()) {
+  when (reset.asBool) {
     buf.foreach(_.valid := false.B)
   }
 }
@@ -79,7 +79,7 @@ class InstDecompressor(rob_entries: Int)(implicit p: Parameters) extends Module
     pushed_preload := false.B
   }
 
-  when (reset.asBool()) {
+  when (reset.asBool) {
     buf.valid := false.B
   }
 }
diff --git a/src/main/scala/gemmini/LocalAddr.scala b/src/main/scala/gemmini/LocalAddr.scala
index b53addea..fdaeffe6 100644
--- a/src/main/scala/gemmini/LocalAddr.scala
+++ b/src/main/scala/gemmini/LocalAddr.scala
@@ -40,8 +40,8 @@ class LocalAddr(sp_banks: Int, sp_bank_entries: Int, acc_banks: Int, acc_bank_en
 
   def is_same_address(other: LocalAddr): Bool = is_acc_addr === other.is_acc_addr && data === other.data
   def is_same_address(other: UInt): Bool = is_same_address(other.asTypeOf(this))
-  def is_garbage(dummy: Int = 0) = is_acc_addr && accumulate && read_full_acc_row && data.andR() &&
-    (if (garbage_bit.getWidth > 0) garbage_bit.asBool() else true.B)
+  def is_garbage(dummy: Int = 0) = is_acc_addr && accumulate && read_full_acc_row && data.andR &&
+    (if (garbage_bit.getWidth > 0) garbage_bit.asBool else true.B)
 
   def +(other: UInt) = {
     require(isPow2(sp_bank_entries)) // TODO remove this requirement
diff --git a/src/main/scala/gemmini/LoopConv.scala b/src/main/scala/gemmini/LoopConv.scala
index 210bcade..53032a51 100644
--- a/src/main/scala/gemmini/LoopConv.scala
+++ b/src/main/scala/gemmini/LoopConv.scala
@@ -174,10 +174,10 @@ class LoopConvLdBias(block_size: Int, coreMaxAddrBits: Int, large_iterator_bitwi
     io.cmd.bits.rs1 := o.dram_addr
     val mvin_cmd_rs2 = Wire(mvin_rs2_t.cloneType)
     mvin_cmd_rs2 := DontCare
-    mvin_cmd_rs2.num_rows := o.I.asUInt()
-    mvin_cmd_rs2.num_cols := o.J.asUInt()
+    mvin_cmd_rs2.num_rows := o.I.asUInt
+    mvin_cmd_rs2.num_cols := o.J.asUInt
     mvin_cmd_rs2.local_addr := cast_to_acc_addr(mvin_cmd_rs2.local_addr, o.spad_addr, accumulate = false.B, read_full = false.B)
-    io.cmd.bits.rs2 := mvin_cmd_rs2.asUInt()
+    io.cmd.bits.rs2 := mvin_cmd_rs2.asUInt
   }
 
   // Sending outputs
@@ -257,8 +257,8 @@ class LoopConvLdInput(block_size: Int, coreMaxAddrBits: Int, large_iterator_bitw
   def undilated(x: UInt): UInt = (x +& req.input_dilated) >> req.input_dilated
 
   // Derived parameters
-  val max_ichs_per_mvin = Mux(ichs < (max_block_len * block_size).U, ichs, (max_block_len * block_size).U).zext()
-  val max_batches_per_mvin = Mux(batches < (max_block_len * block_size).U, batches, (max_block_len * block_size).U).zext()
+  val max_ichs_per_mvin = Mux(ichs < (max_block_len * block_size).U, ichs, (max_block_len * block_size).U).zext
+  val max_batches_per_mvin = Mux(batches < (max_block_len * block_size).U, batches, (max_block_len * block_size).U).zext
   val max_chs_per_mvin = Mux(req.trans_input_3120, max_batches_per_mvin, max_ichs_per_mvin)
 
   // Iterators
@@ -268,34 +268,34 @@ class LoopConvLdInput(block_size: Int, coreMaxAddrBits: Int, large_iterator_bitw
   val ich = Reg(SInt(large_iterator_bitwidth.W))
 
   // Calculated params
-  val irow_padded = irow +& undilated(upad).zext()
-  val icol_padded = icol +& undilated(lpad).zext()
-  val is_zeros = irow < 0.S || irow >= irows_unpadded.zext() || icol < 0.S || icol >= icols_unpadded.zext()
+  val irow_padded = irow +& undilated(upad).zext
+  val icol_padded = icol +& undilated(lpad).zext
+  val is_zeros = irow < 0.S || irow >= irows_unpadded.zext || icol < 0.S || icol >= icols_unpadded.zext
 
   val dram_stride = Mux(req.trans_input_3120, batch_size * (input_w/8).U, in_channels * (input_w/8).U)
 
   // Addresses
-  val dram_offset = Mux(req.trans_input_3120, (((ich * in_dim * in_dim +& irow*in_dim +& icol) * batches +& b) * (input_w/8).U).asUInt(),
-    (((b * in_dim * in_dim +& irow*in_dim +& icol) * in_channels +& ich) * (input_w/8).U).asUInt())
+  val dram_offset = Mux(req.trans_input_3120, (((ich * in_dim * in_dim +& irow*in_dim +& icol) * batches +& b) * (input_w/8).U).asUInt,
+    (((b * in_dim * in_dim +& irow*in_dim +& icol) * in_channels +& ich) * (input_w/8).U).asUInt)
   val dram_addr = Mux(is_zeros, 0.U, req.dram_addr + LoopConv.castDramOffset(dram_offset))
   val spad_addr = Mux(req.trans_input_3120,
     // To prevent Verilator errors, we replace some "/ block_size.U" calls here with ">> log2Up(block_size)"
-    req.addr_start.zext() +& (b >> log2Up(block_size)) * input_spad_stride +& ich * (irows >> req.downsample) * (icols >> req.downsample) +& (irow_padded >> req.downsample) * (icols >> req.downsample) +& (icol_padded >> req.downsample),
-    req.addr_start.zext() +& (ich >> log2Up(block_size)) * input_spad_stride +& b * (irows >> req.downsample) * (icols >> req.downsample) +& (irow_padded >> req.downsample) * (icols >> req.downsample) +& (icol_padded >> req.downsample))
+    req.addr_start.zext +& (b >> log2Up(block_size)) * input_spad_stride +& ich * (irows >> req.downsample) * (icols >> req.downsample) +& (irow_padded >> req.downsample) * (icols >> req.downsample) +& (icol_padded >> req.downsample),
+    req.addr_start.zext +& (ich >> log2Up(block_size)) * input_spad_stride +& b * (irows >> req.downsample) * (icols >> req.downsample) +& (irow_padded >> req.downsample) * (icols >> req.downsample) +& (icol_padded >> req.downsample))
 
   // Sizes
-  val block_size_downsampled = (block_size.U << req.downsample).asUInt().zext()
+  val block_size_downsampled = (block_size.U << req.downsample).asUInt.zext
 
   val I = MuxCase(
-    Mux(icols_unpadded.zext() -& icol > block_size_downsampled, block_size_downsampled, icols_unpadded.zext() -& icol),
+    Mux(icols_unpadded.zext -& icol > block_size_downsampled, block_size_downsampled, icols_unpadded.zext -& icol),
     Seq(
       (icol < 0.S) -> Mux((0.S-&icol) > block_size.S, block_size.S, 0.S-&icol),
-      (icol >= icols_unpadded.zext()) -> Mux(icols_unpadded.zext() +& undilated(rpad).zext() -& icol > block_size.S, block_size.S, icols_unpadded.zext() +& undilated(rpad).zext() -& icol)
+      (icol >= icols_unpadded.zext) -> Mux(icols_unpadded.zext +& undilated(rpad).zext -& icol > block_size.S, block_size.S, icols_unpadded.zext +& undilated(rpad).zext -& icol)
     )
   )
   val K = Mux(req.trans_input_3120,
-    Mux(batches.zext() -& b > max_chs_per_mvin, max_chs_per_mvin, batches.zext() -& b),
-    Mux(ichs.zext() -& ich > max_chs_per_mvin, max_chs_per_mvin, ichs.zext() -& ich))
+    Mux(batches.zext -& b > max_chs_per_mvin, max_chs_per_mvin, batches.zext -& b),
+    Mux(ichs.zext -& ich > max_chs_per_mvin, max_chs_per_mvin, ichs.zext -& ich))
 
   class RoCCCommandWithAddr extends Bundle {
     val cmd = new RoCCCommand
@@ -318,7 +318,7 @@ class LoopConvLdInput(block_size: Int, coreMaxAddrBits: Int, large_iterator_bitw
   config_cmd_rs1.state_id := 0.U
   config_cmd_rs1.shrink := 0.U
   config_cmd_rs1._unused := 1.U
-  config_cmd.rs1 := config_cmd_rs1.asUInt()
+  config_cmd.rs1 := config_cmd_rs1.asUInt
 
   config_cmd.rs2 := dram_stride << req.downsample
 
@@ -348,10 +348,10 @@ class LoopConvLdInput(block_size: Int, coreMaxAddrBits: Int, large_iterator_bitw
     io.cmd.bits.rs1 := o.dram_addr
     val mvin_cmd_rs2 = Wire(mvin_rs2_t.cloneType)
     mvin_cmd_rs2 := DontCare
-    mvin_cmd_rs2.num_rows := (o.I >> req.downsample).asUInt()
-    mvin_cmd_rs2.num_cols := o.K.asUInt()
+    mvin_cmd_rs2.num_rows := (o.I >> req.downsample).asUInt
+    mvin_cmd_rs2.num_cols := o.K.asUInt
     mvin_cmd_rs2.local_addr := cast_to_sp_addr(mvin_cmd_rs2.local_addr, o.spad_addr)
-    io.cmd.bits.rs2 := mvin_cmd_rs2.asUInt()
+    io.cmd.bits.rs2 := mvin_cmd_rs2.asUInt
   }
 
   // Sending outputs
@@ -359,23 +359,23 @@ class LoopConvLdInput(block_size: Int, coreMaxAddrBits: Int, large_iterator_bitw
     when (state === config) {
       state := ld
     }.otherwise {
-      val b_it = Mux(req.trans_input_3120, max_chs_per_mvin.asUInt(), 1.U)
-      val ich_it = Mux(req.trans_input_3120, 1.U, max_chs_per_mvin.asUInt())
+      val b_it = Mux(req.trans_input_3120, max_chs_per_mvin.asUInt, 1.U)
+      val ich_it = Mux(req.trans_input_3120, 1.U, max_chs_per_mvin.asUInt)
 
-      val next_ich = sFloorAdd(ich, ich_it, ichs.zext(), 0.S)
-      val next_icol = sFloorAdd(icol, I.asUInt(), (icols_unpadded +& undilated(rpad)).zext(), 0.S-&undilated(lpad).zext(),
+      val next_ich = sFloorAdd(ich, ich_it, ichs.zext, 0.S)
+      val next_icol = sFloorAdd(icol, I.asUInt, (icols_unpadded +& undilated(rpad)).zext, 0.S-&undilated(lpad).zext,
         next_ich === 0.S)
-      val next_irow = sFloorAdd(irow, 1.U << req.downsample, (irows_unpadded +& undilated(dpad)).zext(), 0.S-&undilated(upad).zext(),
-        next_icol === 0.S-&undilated(lpad).zext() && next_ich === 0.S)
-      val next_b = sFloorAdd(b, b_it, batches.zext(), 0.S,
-        next_irow === 0.S-&undilated(upad).zext() && next_icol === 0.S-&undilated(lpad).zext() && next_ich === 0.S)
+      val next_irow = sFloorAdd(irow, 1.U << req.downsample, (irows_unpadded +& undilated(dpad)).zext, 0.S-&undilated(upad).zext,
+        next_icol === 0.S-&undilated(lpad).zext && next_ich === 0.S)
+      val next_b = sFloorAdd(b, b_it, batches.zext, 0.S,
+        next_irow === 0.S-&undilated(upad).zext && next_icol === 0.S-&undilated(lpad).zext && next_ich === 0.S)
 
       ich := next_ich
       icol := next_icol
       irow := next_irow
       b := next_b
 
-      state := Mux(next_b === 0.S && next_irow === 0.S-&undilated(upad).zext() && next_icol === 0.S-&undilated(lpad).zext() && next_ich === 0.S,
+      state := Mux(next_b === 0.S && next_irow === 0.S-&undilated(upad).zext && next_icol === 0.S-&undilated(lpad).zext && next_ich === 0.S,
         idle, ld)
     }
   }
@@ -385,8 +385,8 @@ class LoopConvLdInput(block_size: Int, coreMaxAddrBits: Int, large_iterator_bitw
     req := io.req.bits
     state := config
     b := 0.S
-    irow := 0.S -& ((io.req.bits.inner_bounds.upad +& io.req.bits.input_dilated) >> io.req.bits.input_dilated).zext()
-    icol := 0.S -& ((io.req.bits.inner_bounds.lpad +& io.req.bits.input_dilated) >> io.req.bits.input_dilated).zext()
+    irow := 0.S -& ((io.req.bits.inner_bounds.upad +& io.req.bits.input_dilated) >> io.req.bits.input_dilated).zext
+    icol := 0.S -& ((io.req.bits.inner_bounds.lpad +& io.req.bits.input_dilated) >> io.req.bits.input_dilated).zext
     ich := 0.S
   }
 }
@@ -530,7 +530,7 @@ class LoopConvLdWeight(block_size: Int, coreMaxAddrBits: Int, large_iterator_bit
     mvin_cmd_rs2.num_rows := o.K
     mvin_cmd_rs2.num_cols := o.J
     mvin_cmd_rs2.local_addr := cast_to_sp_addr(mvin_cmd_rs2.local_addr, o.spad_addr)
-    io.cmd.bits.rs2 := mvin_cmd_rs2.asUInt()
+    io.cmd.bits.rs2 := mvin_cmd_rs2.asUInt
   }
 
   // Sending outputs
@@ -633,8 +633,8 @@ class LoopConvExecute(block_size: Int, large_iterator_bitwidth: Int, small_itera
   val ocol = Reg(UInt(small_iterator_bitwidth.W))
 
   // TODO kernel-dilation and input-dilation can never be activated at the same time, so we can optimize out some multiplications by kernel_dilation
-  val skip_iteration = state >= pre && req.input_dilated && (((krow * kernel_dilation +& orow -& upad)(0) & req.input_dilated).asBool() ||
-    ((kcol * kernel_dilation +& ocol -& lpad)(0) & req.input_dilated).asBool())
+  val skip_iteration = state >= pre && req.input_dilated && (((krow * kernel_dilation +& orow -& upad)(0) & req.input_dilated).asBool ||
+    ((kcol * kernel_dilation +& ocol -& lpad)(0) & req.input_dilated).asBool)
 
   val pixels = Mux(kcols - kcol > req.max_pixels_per_row, req.max_pixels_per_row, kcols - kcol)
 
@@ -643,7 +643,7 @@ class LoopConvExecute(block_size: Int, large_iterator_bitwidth: Int, small_itera
 
   val I = Mux(req.trans_input_3120,
     Mux(batches - b > block_size.U, block_size.U, batches - b),
-    undilated(Mux(ocols - ocol > (block_size.U << req.input_dilated).asUInt(), (block_size.U << req.input_dilated).asUInt(), ocols - ocol)))
+    undilated(Mux(ocols - ocol > (block_size.U << req.input_dilated).asUInt, (block_size.U << req.input_dilated).asUInt, ocols - ocol)))
   val J = Mux(ochs - och > block_size.U, block_size.U, ochs - och)
   val K = pixels * Mux(kchs - kch > block_size.U, block_size.U, kchs - kch)
 
@@ -687,16 +687,16 @@ class LoopConvExecute(block_size: Int, large_iterator_bitwidth: Int, small_itera
 
   val config_cmd_rs1 = Wire(config_ex_rs1_t.cloneType)
   config_cmd_rs1 := DontCare
-  config_cmd_rs1.a_stride := (irows * icols).asUInt()
+  config_cmd_rs1.a_stride := (irows * icols).asUInt
   config_cmd_rs1.set_only_strides := 1.U
   config_cmd_rs1.cmd_type := 0.U
 
   val config_cmd_rs2 = Wire(new ConfigExRs2)
   config_cmd_rs2 := DontCare
-  config_cmd_rs2.c_stride := (orows * ocols).asUInt()
+  config_cmd_rs2.c_stride := (orows * ocols).asUInt
 
-  config_cmd.rs1 := config_cmd_rs1.asUInt()
-  config_cmd.rs2 := config_cmd_rs2.asUInt()
+  config_cmd.rs1 := config_cmd_rs1.asUInt
+  config_cmd.rs2 := config_cmd_rs2.asUInt
 
   val pre_cmd = Wire(new RoCCCommand) // preload
   pre_cmd := DontCare
@@ -735,35 +735,35 @@ class LoopConvExecute(block_size: Int, large_iterator_bitwidth: Int, small_itera
 
     val pre_cmd_rs1 = Wire(preload_rs1_t.cloneType)
     pre_cmd_rs1 := DontCare
-    pre_cmd_rs1.num_rows := o.K.asUInt()
-    pre_cmd_rs1.num_cols := o.J.asUInt()
+    pre_cmd_rs1.num_rows := o.K.asUInt
+    pre_cmd_rs1.num_cols := o.J.asUInt
     pre_cmd_rs1.local_addr := Mux(o.new_weights, cast_to_sp_addr(pre_cmd_rs1.local_addr, o.b_addr),
       garbage_addr(pre_cmd_rs1.local_addr))
 
     val pre_cmd_rs2 = Wire(preload_rs2_t.cloneType)
     pre_cmd_rs2 := DontCare
-    pre_cmd_rs2.num_rows := o.I.asUInt()
-    pre_cmd_rs2.num_cols := o.J.asUInt()
+    pre_cmd_rs2.num_rows := o.I.asUInt
+    pre_cmd_rs2.num_cols := o.J.asUInt
     pre_cmd_rs2.local_addr := cast_to_acc_addr(pre_cmd_rs2.local_addr, o.c_addr, accumulate = true.B, read_full = false.B)
 
-    io.cmd.bits.rs1 := pre_cmd_rs1.asUInt()
-    io.cmd.bits.rs2 := pre_cmd_rs2.asUInt()
+    io.cmd.bits.rs1 := pre_cmd_rs1.asUInt
+    io.cmd.bits.rs2 := pre_cmd_rs2.asUInt
   }.elsewhen(command_p.io.out.bits.cmd.inst.funct =/= CONFIG_CMD) {
     val o = command_p.io.out.bits
     val comp_cmd_rs1 = Wire(compute_rs1_t.cloneType)
     comp_cmd_rs1 := DontCare
-    comp_cmd_rs1.num_rows := o.I.asUInt()
-    comp_cmd_rs1.num_cols := o.K.asUInt()
+    comp_cmd_rs1.num_rows := o.I.asUInt
+    comp_cmd_rs1.num_cols := o.K.asUInt
     comp_cmd_rs1.local_addr := cast_to_sp_addr(comp_cmd_rs1.local_addr, o.a_addr)
 
     val comp_cmd_rs2 = Wire(compute_rs2_t.cloneType)
     comp_cmd_rs2 := DontCare
-    comp_cmd_rs2.num_rows := o.I.asUInt()
-    comp_cmd_rs2.num_cols := o.J.asUInt()
+    comp_cmd_rs2.num_rows := o.I.asUInt
+    comp_cmd_rs2.num_cols := o.J.asUInt
     comp_cmd_rs2.local_addr := garbage_addr(comp_cmd_rs2.local_addr)
 
-    io.cmd.bits.rs1 := comp_cmd_rs1.asUInt()
-    io.cmd.bits.rs2 := comp_cmd_rs2.asUInt()
+    io.cmd.bits.rs1 := comp_cmd_rs1.asUInt
+    io.cmd.bits.rs2 := comp_cmd_rs2.asUInt
   }
 
   // Updating "new_weights"
@@ -779,7 +779,7 @@ class LoopConvExecute(block_size: Int, large_iterator_bitwidth: Int, small_itera
       state := comp
     }.otherwise {
       val b_it = Mux(req.trans_input_3120, block_size.U, 1.U)
-      val ocol_it = Mux(skip_iteration || req.trans_input_3120, 1.U, block_size.U << req.input_dilated).asUInt()
+      val ocol_it = Mux(skip_iteration || req.trans_input_3120, 1.U, block_size.U << req.input_dilated).asUInt
 
       val next_ocol = floorAdd(ocol, ocol_it, ocols)
       val next_orow = floorAdd(orow, 1.U, orows, next_ocol === 0.U)
@@ -928,13 +928,13 @@ class LoopConvSt(block_size: Int, coreMaxAddrBits: Int, large_iterator_bitwidth:
   pre_pool_config_cmd_rs1.pool_stride := pool_stride
   pre_pool_config_cmd_rs1.activation := req.activation
   pre_pool_config_cmd_rs1.cmd_type := CONFIG_STORE
-  pre_pool_config_cmd.rs1 := pre_pool_config_cmd_rs1.asUInt()
+  pre_pool_config_cmd.rs1 := pre_pool_config_cmd_rs1.asUInt
 
   val pre_pool_config_cmd_rs2 = Wire(config_mvout_rs2_t.cloneType)
   pre_pool_config_cmd_rs2 := DontCare
   pre_pool_config_cmd_rs2.acc_scale := ACC_SCALE_NO_CHANGE
   pre_pool_config_cmd_rs2.stride := out_channels * (input_w / 8).U
-  pre_pool_config_cmd.rs2 := pre_pool_config_cmd_rs2.asUInt()
+  pre_pool_config_cmd.rs2 := pre_pool_config_cmd_rs2.asUInt
 
   val post_pool_config_cmd = Wire(new RoCCCommand)
   post_pool_config_cmd := DontCare
@@ -944,13 +944,13 @@ class LoopConvSt(block_size: Int, coreMaxAddrBits: Int, large_iterator_bitwidth:
   post_pool_config_cmd_rs1 := DontCare
   post_pool_config_cmd_rs1.activation := req.activation
   post_pool_config_cmd_rs1.cmd_type := CONFIG_STORE
-  post_pool_config_cmd.rs1 := post_pool_config_cmd_rs1.asUInt()
+  post_pool_config_cmd.rs1 := post_pool_config_cmd_rs1.asUInt
 
   val post_pool_config_cmd_rs2 = Wire(config_mvout_rs2_t.cloneType)
   post_pool_config_cmd_rs2 := DontCare
   post_pool_config_cmd_rs2.acc_scale := ACC_SCALE_NO_CHANGE
   post_pool_config_cmd_rs2.stride := out_channels * (input_w / 8).U
-  post_pool_config_cmd.rs2 := post_pool_config_cmd_rs2.asUInt()
+  post_pool_config_cmd.rs2 := post_pool_config_cmd_rs2.asUInt
 
   val pool_cmd = Wire(new RoCCCommand)
   pool_cmd := DontCare
@@ -990,16 +990,16 @@ class LoopConvSt(block_size: Int, coreMaxAddrBits: Int, large_iterator_bitwidth:
       pool_mvout_cmd_rs2.local_addr := cast_to_acc_addr(pool_mvout_cmd_rs2.local_addr, o.pool_spad_addr, accumulate = false.B, read_full = false.B)
 
       io.cmd.bits.rs1 := o.pool_dram_addr
-      io.cmd.bits.rs2 := pool_mvout_cmd_rs2.asUInt()
+      io.cmd.bits.rs2 := pool_mvout_cmd_rs2.asUInt
     } .otherwise {
       val mvout_cmd_rs2 = Wire(mvout_rs2_t.cloneType)
       mvout_cmd_rs2 := DontCare
-      mvout_cmd_rs2.num_rows := o.I.asUInt()
-      mvout_cmd_rs2.num_cols := o.J.asUInt()
+      mvout_cmd_rs2.num_rows := o.I.asUInt
+      mvout_cmd_rs2.num_cols := o.J.asUInt
       mvout_cmd_rs2.local_addr := cast_to_acc_addr(mvout_cmd_rs2.local_addr, o.spad_addr, accumulate = false.B, read_full = false.B)
 
       io.cmd.bits.rs1 := o.dram_addr
-      io.cmd.bits.rs2 := mvout_cmd_rs2.asUInt()
+      io.cmd.bits.rs2 := mvout_cmd_rs2.asUInt
     }
   }
 
@@ -1182,7 +1182,7 @@ class LoopConv (block_size: Int, coreMaxAddrBits: Int, reservation_station_size:
   val concurrent_loops = 2
   val loops = Reg(Vec(concurrent_loops, new LoopConvState(block_size, large_iterator_bitwidth, small_iterator_bitwidth, tiny_iterator_bitwidth, coreMaxAddrBits, max_addr, max_acc_addr)))
   val head_loop_id = RegInit(0.U(log2Up(concurrent_loops).W))
-  val tail_loop_id = (~head_loop_id).asUInt() // This is the loop that we always try to configure if available
+  val tail_loop_id = (~head_loop_id).asUInt // This is the loop that we always try to configure if available
   val head_loop = loops(head_loop_id)
   val tail_loop = loops(tail_loop_id)
 
@@ -1499,7 +1499,7 @@ class LoopConv (block_size: Int, coreMaxAddrBits: Int, reservation_station_size:
   }
 
   // Resets
-  when (reset.asBool()) {
+  when (reset.asBool) {
     loops.zipWithIndex.foreach { case (l, i) =>
       l.reset()
       l.a_addr_start := (i * (max_addr / concurrent_loops)).U
diff --git a/src/main/scala/gemmini/LoopMatmul.scala b/src/main/scala/gemmini/LoopMatmul.scala
index 86552d56..a33155e9 100644
--- a/src/main/scala/gemmini/LoopMatmul.scala
+++ b/src/main/scala/gemmini/LoopMatmul.scala
@@ -76,10 +76,10 @@ class LoopMatmulLdA(block_size: Int, coreMaxAddrBits: Int, iterator_bitwidth: In
 
   val mvin_cmd_rs2 = Wire(mvin_rs2_t.cloneType)
   mvin_cmd_rs2 := DontCare
-  mvin_cmd_rs2.num_rows := rows.asUInt()
-  mvin_cmd_rs2.num_cols := cols.asUInt()
+  mvin_cmd_rs2.num_rows := rows.asUInt
+  mvin_cmd_rs2.num_cols := cols.asUInt
   mvin_cmd_rs2.local_addr := cast_to_sp_addr(mvin_cmd_rs2.local_addr, sp_addr)
-  mvin_cmd.rs2 := mvin_cmd_rs2.asUInt()
+  mvin_cmd.rs2 := mvin_cmd_rs2.asUInt
 
   io.req.ready := state === idle
   io.i := i
@@ -184,10 +184,10 @@ class LoopMatmulLdB(block_size: Int, coreMaxAddrBits: Int, iterator_bitwidth: In
 
   val mvin_cmd_rs2 = Wire(mvin_rs2_t.cloneType)
   mvin_cmd_rs2 := DontCare
-  mvin_cmd_rs2.num_rows := rows.asUInt()
-  mvin_cmd_rs2.num_cols := cols.asUInt()
+  mvin_cmd_rs2.num_rows := rows.asUInt
+  mvin_cmd_rs2.num_cols := cols.asUInt
   mvin_cmd_rs2.local_addr := cast_to_sp_addr(mvin_cmd_rs2.local_addr, sp_addr)
-  mvin_cmd.rs2 := mvin_cmd_rs2.asUInt()
+  mvin_cmd.rs2 := mvin_cmd_rs2.asUInt
 
   io.req.ready := state === idle
   io.k := k
@@ -281,10 +281,10 @@ class LoopMatmulLdD(block_size: Int, coreMaxAddrBits: Int, iterator_bitwidth: In
 
   val mvin_cmd_rs2 = Wire(mvin_rs2_t.cloneType)
   mvin_cmd_rs2 := DontCare
-  mvin_cmd_rs2.num_rows := rows.asUInt()
-  mvin_cmd_rs2.num_cols := cols.asUInt()
+  mvin_cmd_rs2.num_rows := rows.asUInt
+  mvin_cmd_rs2.num_cols := cols.asUInt
   mvin_cmd_rs2.local_addr := cast_to_acc_addr(mvin_cmd_rs2.local_addr, sp_addr, accumulate = false.B, read_full = false.B)
-  mvin_cmd.rs2 := mvin_cmd_rs2.asUInt()
+  mvin_cmd.rs2 := mvin_cmd_rs2.asUInt
 
   io.req.ready := state === idle
   io.idle := state === idle
@@ -401,19 +401,19 @@ class LoopMatmulExecute(block_size: Int, coreMaxAddrBits: Int, iterator_bitwidth
 
   val pre_cmd_rs1 = Wire(preload_rs1_t.cloneType)
   pre_cmd_rs1 := DontCare
-  pre_cmd_rs1.num_rows := b_rows.asUInt()
-  pre_cmd_rs1.num_cols := b_cols.asUInt()
+  pre_cmd_rs1.num_rows := b_rows.asUInt
+  pre_cmd_rs1.num_cols := b_cols.asUInt
   pre_cmd_rs1.local_addr := Mux(i === 0.U, cast_to_sp_addr(pre_cmd_rs1.local_addr, b_addr),
     garbage_addr(pre_cmd_rs1.local_addr))
 
   val pre_cmd_rs2 = Wire(preload_rs2_t.cloneType)
   pre_cmd_rs2 := DontCare
-  pre_cmd_rs2.num_rows := c_rows.asUInt()
-  pre_cmd_rs2.num_cols := c_cols.asUInt()
+  pre_cmd_rs2.num_rows := c_rows.asUInt
+  pre_cmd_rs2.num_cols := c_cols.asUInt
   pre_cmd_rs2.local_addr := cast_to_acc_addr(pre_cmd_rs2.local_addr, c_addr, accumulate = req.accumulate || k =/= 0.U, read_full = false.B)
 
-  pre_cmd.rs1 := pre_cmd_rs1.asUInt()
-  pre_cmd.rs2 := pre_cmd_rs2.asUInt()
+  pre_cmd.rs1 := pre_cmd_rs1.asUInt
+  pre_cmd.rs2 := pre_cmd_rs2.asUInt
 
   val comp_cmd = Wire(new RoCCCommand())
   comp_cmd := DontCare
@@ -421,8 +421,8 @@ class LoopMatmulExecute(block_size: Int, coreMaxAddrBits: Int, iterator_bitwidth
 
   val comp_cmd_rs1 = Wire(compute_rs1_t.cloneType)
   comp_cmd_rs1 := DontCare
-  comp_cmd_rs1.num_rows := a_rows.asUInt()
-  comp_cmd_rs1.num_cols := a_cols.asUInt()
+  comp_cmd_rs1.num_rows := a_rows.asUInt
+  comp_cmd_rs1.num_cols := a_cols.asUInt
   comp_cmd_rs1.local_addr := cast_to_sp_addr(comp_cmd_rs1.local_addr, a_addr)
 
   val comp_cmd_rs2 = Wire(compute_rs2_t.cloneType)
@@ -431,8 +431,8 @@ class LoopMatmulExecute(block_size: Int, coreMaxAddrBits: Int, iterator_bitwidth
   comp_cmd_rs2.num_cols := block_size.U
   comp_cmd_rs2.local_addr := garbage_addr(comp_cmd_rs2.local_addr)
 
-  comp_cmd.rs1 := comp_cmd_rs1.asUInt()
-  comp_cmd.rs2 := comp_cmd_rs2.asUInt()
+  comp_cmd.rs1 := comp_cmd_rs1.asUInt
+  comp_cmd.rs2 := comp_cmd_rs2.asUInt
 
   io.req.ready := state === idle
   io.k := k
@@ -528,7 +528,7 @@ class LoopMatmulStC(block_size: Int, coreMaxAddrBits: Int, iterator_bitwidth: In
   val j = Reg(UInt(iterator_bitwidth.W))
   val i = Reg(UInt(iterator_bitwidth.W))
 
-  val acc_addr_start = /*(BigInt(1) << 31).U | (req.full_c << 29.U).asUInt() |*/ req.addr_start
+  val acc_addr_start = /*(BigInt(1) << 31).U | (req.full_c << 29.U).asUInt |*/ req.addr_start
 
   val dram_offset = Mux(req.full_c, (i * req.dram_stride + j) * block_size.U * (acc_w/8).U,
     (i * req.dram_stride + j) * block_size.U * (input_w/8).U)
@@ -545,10 +545,10 @@ class LoopMatmulStC(block_size: Int, coreMaxAddrBits: Int, iterator_bitwidth: In
 
   val mvout_cmd_rs2 = Wire(mvout_rs2_t.cloneType)
   mvout_cmd_rs2 := DontCare
-  mvout_cmd_rs2.num_rows := rows.asUInt()
-  mvout_cmd_rs2.num_cols := cols.asUInt()
+  mvout_cmd_rs2.num_rows := rows.asUInt
+  mvout_cmd_rs2.num_cols := cols.asUInt
   mvout_cmd_rs2.local_addr := cast_to_acc_addr(mvout_cmd_rs2.local_addr, sp_addr, accumulate = false.B, read_full = req.full_c)
-  mvout_cmd.rs2 := mvout_cmd_rs2.asUInt()
+  mvout_cmd.rs2 := mvout_cmd_rs2.asUInt
 
   // Layernorm iterators and calculations
   val ln_row = Reg(UInt(iterator_bitwidth.W))
@@ -585,7 +585,7 @@ class LoopMatmulStC(block_size: Int, coreMaxAddrBits: Int, iterator_bitwidth: In
   val ln_config_norm = Wire(new RoCCCommand)
   ln_config_norm := DontCare
   ln_config_norm.inst.funct := CONFIG_CMD
-  ln_config_norm.rs1 := ln_config_norm_rs1.asUInt()
+  ln_config_norm.rs1 := ln_config_norm_rs1.asUInt
   ln_config_norm.rs2 := DontCare
 
   val ln_mvout_cmd = Wire(new RoCCCommand)
@@ -596,10 +596,10 @@ class LoopMatmulStC(block_size: Int, coreMaxAddrBits: Int, iterator_bitwidth: In
   val ln_mvout_cmd_rs2 = Wire(mvout_rs2_t.cloneType)
   ln_mvout_cmd_rs2 := DontCare
   ln_mvout_cmd_rs2.num_rows := 1.U
-  ln_mvout_cmd_rs2.num_cols := cols.asUInt()
+  ln_mvout_cmd_rs2.num_cols := cols.asUInt
   ln_mvout_cmd_rs2.local_addr := cast_to_acc_addr(ln_mvout_cmd_rs2.local_addr, ln_sp_addr, accumulate = false.B, read_full = req.full_c)
   ln_mvout_cmd_rs2.local_addr.norm_cmd := ln_norm_cmd
-  ln_mvout_cmd.rs2 := ln_mvout_cmd_rs2.asUInt()
+  ln_mvout_cmd.rs2 := ln_mvout_cmd_rs2.asUInt
 
   io.req.ready := state === idle
   io.j := j
@@ -760,7 +760,7 @@ class LoopMatmul(block_size: Int, coreMaxAddrBits: Int, reservation_station_size
   val concurrent_loops = 2
   val loops = Reg(Vec(concurrent_loops, new LoopMatmulState(iterator_bitwidth, coreMaxAddrBits, max_addr, max_acc_addr)))
   val head_loop_id = Reg(UInt(log2Up(concurrent_loops).W))
-  val tail_loop_id = (~head_loop_id).asUInt() // This is the loop that we always try to configure if available
+  val tail_loop_id = (~head_loop_id).asUInt // This is the loop that we always try to configure if available
   val head_loop = loops(head_loop_id)
   val tail_loop = loops(tail_loop_id)
 
@@ -1052,7 +1052,7 @@ class LoopMatmul(block_size: Int, coreMaxAddrBits: Int, reservation_station_size
   }
 
   // Resets
-  when (reset.asBool()) {
+  when (reset.asBool) {
     loops.zipWithIndex.foreach { case (l, i) =>
       l.reset()
       l.a_addr_start := (i * (max_addr / concurrent_loops)).U
diff --git a/src/main/scala/gemmini/LoopUnroller.scala b/src/main/scala/gemmini/LoopUnroller.scala
index 02ac7d71..63a0150b 100644
--- a/src/main/scala/gemmini/LoopUnroller.scala
+++ b/src/main/scala/gemmini/LoopUnroller.scala
@@ -42,8 +42,8 @@ class LoopUnroller(block_size: Int)(implicit p: Parameters) extends Module {
 
   val a_start = cmd.bits.rs1(31, 0)
   val b_start = cmd.bits.rs1(63, 32)
-  val c_start = (3.U << 30).asUInt()
-  val d_start = (1.U << 31).asUInt()
+  val c_start = (3.U << 30).asUInt
+  val d_start = (1.U << 31).asUInt
 
   // TODO get rid of the x * max_y multiplications here
   val a_addr = a_start + (i * max_k + k) * block_size.U
diff --git a/src/main/scala/gemmini/MeshWithDelays.scala b/src/main/scala/gemmini/MeshWithDelays.scala
index d0aced16..516760bf 100644
--- a/src/main/scala/gemmini/MeshWithDelays.scala
+++ b/src/main/scala/gemmini/MeshWithDelays.scala
@@ -248,7 +248,7 @@ class MeshWithDelays[T <: Data: Arithmetic, U <: TagQueueTag with Data]
   io.req.ready := (!req.valid || last_fire) && tagq.io.enq.ready && total_rows_q.io.enq.ready
   io.tags_in_progress := tagq.io.all.map(_.tag)
 
-  when (reset.asBool()) {
+  when (reset.asBool) {
     req.valid := false.B
   }
 
diff --git a/src/main/scala/gemmini/Normalizer.scala b/src/main/scala/gemmini/Normalizer.scala
index 89dca2db..67dd18ac 100644
--- a/src/main/scala/gemmini/Normalizer.scala
+++ b/src/main/scala/gemmini/Normalizer.scala
@@ -351,7 +351,7 @@ class Normalizer[T <: Data, U <: Data](max_len: Int, num_reduce_lanes: Int, num_
     assert(acc_t.isInstanceOf[SInt])
 
     when (stat.state === waiting_for_stddev) {
-      stat.inv_stddev := Mux(sqrt_out.bits.asUInt() === acc_t.zero.asUInt(),
+      stat.inv_stddev := Mux(sqrt_out.bits.asUInt === acc_t.zero.asUInt,
         1.S(acc_t.getWidth.W).asTypeOf(acc_t),
         sqrt_out.bits
       )
@@ -405,14 +405,14 @@ class Normalizer[T <: Data, U <: Data](max_len: Int, num_reduce_lanes: Int, num_
       def in_to_float(x: SInt) = {
         val in_to_rec_fn = Module(new INToRecFN(intWidth = sum_exp_to_inv.getWidth, expWidth, sigWidth))
         in_to_rec_fn.io.signedIn := true.B
-        in_to_rec_fn.io.in := x.asUInt()
+        in_to_rec_fn.io.in := x.asUInt
         in_to_rec_fn.io.roundingMode := consts.round_near_even // consts.round_near_maxMag
         in_to_rec_fn.io.detectTininess := consts.tininess_afterRounding
 
         in_to_rec_fn.io.out
       }
 
-      val self_rec = in_to_float(sum_exp_to_inv.asUInt().asSInt())
+      val self_rec = in_to_float(sum_exp_to_inv.asUInt.asSInt)
       val one_rec = in_to_float(127.S) // softmax maximum is 127 for signed int8
 
       // Instantiate the hardloat divider
@@ -436,7 +436,7 @@ class Normalizer[T <: Data, U <: Data](max_len: Int, num_reduce_lanes: Int, num_
     val stat = stats(sum_exp_to_inv_id)
 
     exp_divider_in.valid := (stat.state === get_inv_sum_exp) && !lanes.io.busy
-    exp_divider_in.bits := sum_exp_to_inv.asUInt()
+    exp_divider_in.bits := sum_exp_to_inv.asUInt
   }
 
   {
@@ -587,7 +587,7 @@ class Normalizer[T <: Data, U <: Data](max_len: Int, num_reduce_lanes: Int, num_
   assert(acc_t.getWidth == scale_t.getWidth, "we use the same variable to hold both the variance and the inv-stddev, so we need them to see the width")
 
   // Resets
-  when (reset.asBool()) {
+  when (reset.asBool) {
     stats.foreach(_.state := idle)
     stats.foreach(_.sum := acc_t.zero)
     stats.foreach(_.max := acc_t.minimum)
diff --git a/src/main/scala/gemmini/PixelRepeater.scala b/src/main/scala/gemmini/PixelRepeater.scala
index ddab4422..ecf9481c 100644
--- a/src/main/scala/gemmini/PixelRepeater.scala
+++ b/src/main/scala/gemmini/PixelRepeater.scala
@@ -48,8 +48,8 @@ class PixelRepeater[T <: Data, Tag <: Data](t: T, laddr_t: LocalAddr, block_cols
     val out_shift = Wire(UInt(log2Up(block_cols / 2 + 1).W))
     out_shift := req.bits.pixel_repeats * req.bits.len
 
-    io.resp.bits.out := (req.bits.in.asUInt() << (out_shift * t.getWidth.U)).asTypeOf(io.resp.bits.out)
-    io.resp.bits.mask := (req.bits.mask.asUInt() << (out_shift * ((t.getWidth / 8) / aligned_to).U)).asTypeOf(io.resp.bits.mask)
+    io.resp.bits.out := (req.bits.in.asUInt << (out_shift * t.getWidth.U)).asTypeOf(io.resp.bits.out)
+    io.resp.bits.mask := (req.bits.mask.asUInt << (out_shift * ((t.getWidth / 8) / aligned_to).U)).asTypeOf(io.resp.bits.mask)
 
     io.resp.bits.last := req.bits.last && (req.bits.pixel_repeats === 0.U)
     io.resp.bits.tag := req.bits.tag
@@ -84,7 +84,7 @@ class PixelRepeater[T <: Data, Tag <: Data](t: T, laddr_t: LocalAddr, block_cols
       req.bits.pixel_repeats := io.req.bits.pixel_repeats - 1.U
     }
 
-    when(reset.asBool()) {
+    when(reset.asBool) {
       req.pop()
     }
   }
diff --git a/src/main/scala/gemmini/ReservationStation.scala b/src/main/scala/gemmini/ReservationStation.scala
index 68d0e6e7..47dd5ef1 100644
--- a/src/main/scala/gemmini/ReservationStation.scala
+++ b/src/main/scala/gemmini/ReservationStation.scala
@@ -558,7 +558,7 @@ class ReservationStation[T <: Data : Arithmetic, U <: Data, V <: Data](config: G
     PerfCounter(!io.alloc.ready, "reservation_station_full", "cycles where reservation station is full")
   }
 
-  when (reset.asBool()) {
+  when (reset.asBool) {
     entries.foreach(_.valid := false.B)
   }
 
diff --git a/src/main/scala/gemmini/Scratchpad.scala b/src/main/scala/gemmini/Scratchpad.scala
index 70c9140f..91870c80 100644
--- a/src/main/scala/gemmini/Scratchpad.scala
+++ b/src/main/scala/gemmini/Scratchpad.scala
@@ -149,9 +149,9 @@ class ScratchpadBank(n: Int, w: Int, aligned_to: Int, single_ported: Boolean, us
   val ren = io.read.req.fire
   val rdata = if (single_ported) {
     assert(!(ren && io.write.en))
-    read(raddr, ren && !io.write.en).asUInt()
+    read(raddr, ren && !io.write.en).asUInt
   } else {
-    read(raddr, ren).asUInt()
+    read(raddr, ren).asUInt
   }
 
   val fromDMA = io.read.req.bits.fromDMA
@@ -537,7 +537,7 @@ class Scratchpad[T <: Data, U <: Data, V <: Data](config: GemminiArrayConfig[T,
           bio.write.mask := io.srams.write(i).mask
         }.elsewhen (dmaread) {
           bio.write.addr := laddr.sp_row()
-          bio.write.data := mvin_scale_pixel_repeater.io.resp.bits.out.asUInt()
+          bio.write.data := mvin_scale_pixel_repeater.io.resp.bits.out.asUInt
           bio.write.mask := mvin_scale_pixel_repeater.io.resp.bits.mask take ((spad_w / (aligned_to * 8)) max 1)
 
           mvin_scale_pixel_repeater.io.resp.ready := true.B // TODO we combinationally couple valid and ready signals
diff --git a/src/main/scala/gemmini/StoreController.scala b/src/main/scala/gemmini/StoreController.scala
index c9e4fdbb..bf4a71b2 100644
--- a/src/main/scala/gemmini/StoreController.scala
+++ b/src/main/scala/gemmini/StoreController.scala
@@ -226,7 +226,7 @@ class StoreController[T <: Data : Arithmetic, U <: Data, V <: Data](config: Gemm
           stride := config_stride
 
           activation := config_activation
-          when (!config_acc_scale.asUInt().andR()) {
+          when (!config_acc_scale.asUInt.andR) {
             acc_scale := config_acc_scale.asTypeOf(acc_scale_t)
           }
 
@@ -248,7 +248,7 @@ class StoreController[T <: Data : Arithmetic, U <: Data, V <: Data](config: Gemm
           cmd.ready := true.B
         }
         .elsewhen(config.has_normalizations.B && DoConfigNorm) {
-          when (!config_set_stats_id_only.asBool()) {
+          when (!config_set_stats_id_only.asBool) {
             igelu_qb := config_igelu_qb.asTypeOf(igelu_qb)
             igelu_qc := config_igelu_qc.asTypeOf(igelu_qc)
             when(config_iexp_q_const_type === 0.U) {
diff --git a/src/main/scala/gemmini/TagQueue.scala b/src/main/scala/gemmini/TagQueue.scala
index 9a6464c3..f656119a 100644
--- a/src/main/scala/gemmini/TagQueue.scala
+++ b/src/main/scala/gemmini/TagQueue.scala
@@ -44,7 +44,7 @@ class TagQueue[T <: Data with TagQueueTag](t: T, entries: Int) extends Module {
     len := len - 1.U
   }
 
-  when (reset.asBool()) {
+  when (reset.asBool) {
     regs.foreach(_.make_this_garbage())
   }
 
diff --git a/src/main/scala/gemmini/TilerScheduler.scala b/src/main/scala/gemmini/TilerScheduler.scala
index c09ff949..d9fd8b6a 100644
--- a/src/main/scala/gemmini/TilerScheduler.scala
+++ b/src/main/scala/gemmini/TilerScheduler.scala
@@ -253,7 +253,7 @@ class TilerScheduler[T <: Data: Arithmetic, U <: Data, V <: Data]
                        Cat(older_in_same_q) |
                        Cat(is_st_and_must_wait_for_prior_ex_config) |
                        Cat(is_ex_config_and_must_wait_for_prior_st)
-                      ).asBools().reverse
+                      ).asBools.reverse
 
     new_entry.complete_on_issue := new_entry.is_config && new_entry.q =/= exq
 
@@ -440,7 +440,7 @@ class TilerScheduler[T <: Data: Arithmetic, U <: Data, V <: Data]
     printf(p"Last allocated: $last_allocated\n\n")
   }
 
-  when (reset.asBool()) {
+  when (reset.asBool) {
     entries.foreach(_.valid := false.B)
   }
 }
diff --git a/src/main/scala/gemmini/Util.scala b/src/main/scala/gemmini/Util.scala
index 51dc1377..07c36554 100644
--- a/src/main/scala/gemmini/Util.scala
+++ b/src/main/scala/gemmini/Util.scala
@@ -47,9 +47,9 @@ object Util {
   def sFloorAdd(s: SInt, n: UInt, max_plus_one: SInt, min: SInt, en: Bool = true.B): SInt = {
     val max = max_plus_one - 1.S
 
-    MuxCase(s + n.zext(), Seq(
+    MuxCase(s + n.zext, Seq(
       (!en) -> s,
-      ((s +& n.zext()) > max) -> min
+      ((s +& n.zext) > max) -> min
     ))
   }
 
@@ -66,22 +66,22 @@ object Util {
 
   def closestLowerPowerOf2(u: UInt): UInt = {
     // TODO figure out a more efficient way of doing this. Is this many muxes really necessary?
-    val exp = u.asBools().zipWithIndex.map { case (b, i) =>
+    val exp = u.asBools.zipWithIndex.map { case (b, i) =>
         Mux(b, i.U, 0.U)
     }.reduce((acc, u) => Mux(acc > u, acc, u))
 
-    (1.U << exp).asUInt()
+    (1.U << exp).asUInt
   }
 
   def closestAlignedLowerPowerOf2(u: UInt, addr: UInt, stride: UInt, rowBytes: Int): UInt = {
     val lgRowBytes = log2Ceil(rowBytes)
 
     // TODO figure out a more efficient way of doing this. Is this many muxes really necessary?
-    val exp = u.asBools().zipWithIndex.map { case (b, i) =>
+    val exp = u.asBools.zipWithIndex.map { case (b, i) =>
       Mux(b && addr(i + lgRowBytes - 1, 0) === 0.U && stride(i + lgRowBytes - 1, 0) === 0.U, i.U, 0.U)
     }.reduce((acc, u) => Mux(acc > u, acc, u))
 
-    (1.U << exp).asUInt()
+    (1.U << exp).asUInt
   }
 
   // This function will return "next" with a 0-cycle delay when the "enable" signal is high. It's like a queue with
diff --git a/src/main/scala/gemmini/XactTracker.scala b/src/main/scala/gemmini/XactTracker.scala
index 277626a1..afd8f964 100644
--- a/src/main/scala/gemmini/XactTracker.scala
+++ b/src/main/scala/gemmini/XactTracker.scala
@@ -84,7 +84,7 @@ class XactTracker[U <: Data](nXacts: Int, maxShift: Int, spadWidth: Int, accWidt
     assert(entries(io.peek.xactid).valid)
   }
 
-  when (reset.asBool()) {
+  when (reset.asBool) {
     entries.foreach(_.valid := false.B)
   }
 
diff --git a/src/main/scala/gemmini/ZeroWriter.scala b/src/main/scala/gemmini/ZeroWriter.scala
index a1834a41..17da09e1 100644
--- a/src/main/scala/gemmini/ZeroWriter.scala
+++ b/src/main/scala/gemmini/ZeroWriter.scala
@@ -70,7 +70,7 @@ class ZeroWriter[T <: Data, U <: Data, V <: Data, Tag <: Data](config: GemminiAr
     col_counter := 0.U
   }
 
-  when (reset.asBool()) {
+  when (reset.asBool) {
     req.pop()
   }
 }
